[
  {
    "objectID": "Binning.html",
    "href": "Binning.html",
    "title": "Genomas a partir de metagenomas",
    "section": "",
    "text": "La metagen√≥mica hace referencia a todo el ADN de los organismos que se encuentran en un ambiente. La secuenciaci√≥n de este material gen√©tico produce lecturas que pueden ensamblarse para conocer la diversidad microbiana y sus funciones.\nT√≠picamente los metagenomas pueden estudiarse mediante dos aproximaciones:\nEn este apartado nos enfocaremos en la segunda aproximaci√≥n. Los MAGs se reconstruyen a partir de un ensamble metagen√≥mico, los contigs de dicho ensamble se agrupan mediante la informaci√≥n de cobertura y frecuencia de tetranucle√≥tidos. Esta agrupaci√≥n puede generar errores, por lo que es indispensable evaluar la calidad de los MAGs mediante la completitud y redundancia de genes de copia √∫nica (MerenLab y col.)\nPara obtener MAGs podemos seguir el siguiente flujo de an√°lisis:"
  },
  {
    "objectID": "Binning.html#binning",
    "href": "Binning.html#binning",
    "title": "Genomas a partir de metagenomas",
    "section": "Binning",
    "text": "Binning\nEn la secci√≥n anterior aprendimos como evaluar la calidad, filtrar las lecturas y a ensamblarlas, por lo que este apartado comenzar√° con el ensamble ya generado.\nDe acuerdo con el flujo de an√°lisis (Figura 2), debemos partir de un ensamble y mapear las lecturas a dicho ensamble para obtener un archivo de profundidad de cada contig en el ensamble.\n\n\n\n\n\n\nNota\n\n\n\nEl proceso de mapeo es demandante en tiempo de ejecuci√≥n y recursos. As√≠ que nos dimos a la tarea de generar el archivo de profundidad para comenzar directamente con el binning.\nEl mapeo lo corrimos con bowtie2 que es una herramienta confiable y muy utilizada para alinear lecturas cortas a una referencia, en nuestro caso, la referencia es el ensamble metagen√≥mico de la muestra de 48hrs. Bowtie2 genera un archivo de mapeo (SAM) que debe convertirse a un formato binario (BAM), para esta conversi√≥n usamos samtools que contiene multiples subherramientas para trabajar con archivos de mapeos.\nPara generar este archivo se utilizaron las siguientes lineas de c√≥digo.\n# Formatear el ensamble\nbowtie2-build results/02.ensambles/megahit/48hrs/48hrs.fasta results/03.profundidad/48hrs --threads 40\n# Mapear las lecturas contra el ensamble\nbowtie2 -x results/03.profundidad/48hrs -1 data/48hrs_sm_R1.fastq -2 data/48hrs_sm_R2.fastq -p 40 -S results/03.profundidad/48hrs.sam\n# Convertir de SAM a BAM y ordenar\nsamtools view -Sb -O BAM -@ 40 results/03.profundidad/48hrs.sam | samtools sort -@ 40  -o results/03.profundidad/48hrs_sorted.bam\n# Obtener el √≠ndice\nsamtools index results/03.profundidad/48hrs_sorted.bam\nYa que generamos el archivo bam ordenado y el √≠ndice obtuvimos un archivo con la informaci√≥n de cobertura de cada contig dentro del ensamble, este archivo de profundidad se gener√≥ con jgi_summarize_bam_contig_depths que es una herramienta de metabat.\n# jgi\n#Obtener el archivo de profundidad de cada contig\njgi_summarize_bam_contig_depths  --outputDepth results/03.profundidad/48hrs.mgh_depth.txt results/03.profundidad/48hrs_sorted.bam\n\n\n\n\n\n\nAtenci√≥n\n\n\n\nNo las ejecutes, s√≥lo son un ejemplo para que las puedas usar con tus propios datos en el futuro.\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 1\n\n\n\nAntes de comenzar, re√∫nete con tu equipo y juntos\n\nrevisen el contenido de los directorios 02.ensambles y 03.profundidad.txt\nDiscutan y en una diapositiva expliquen el flujo que se sigui√≥ para obtener los archivos que estan esos directorios\n\nLa liga de drive para ir trabajando durante el taller es esta: https://drive.google.com/drive/folders/1iKfhMz_JdfImmsCmkPg10r-NC-nrzhQ4?usp=sharing\n\n\nS√≥lo por si te pierdes\n\n\n\n\n\n\nDirectorio de trabajo\n\n\n\nSi en alg√∫n momento te pierdes entre directorios, puedes regresar al espacio principal asi:\ncd && cd taller_metagenomica_pozol/\n\n\n\n\nMetabat2\nMetabat2 es una herramienta que agrupa los contigs tomando la cobertura de cada contig y calcula su composici√≥n nucleot√≠dica.\nPara correr metabat necesitamos activar el ambiente conda donde se aloja.\n\n\n\n\n\n\nActivar ambiente para Metabat2\n\n\n\n\nbetterlab\nconda activate binning\n\n\n\n\n\n\nMetabat2. Kang et al., 2015. DOI:10.7717/peerj.1165\n\n\nAhora que ya tenemos el ambiente activado ejecutemos la siguiente linea:\nmetabat2 -i results/02.ensambles/48hrs.fasta -a results/03.profundidad/48hrs.mgh_depth.txt -o results/04.metabat/metabat -t 4 -m 1500\n\n\n\n\n\n\nResponde:\n\n\n\n\n\n\n¬øCu√°ntos bins se formaron?\n\n2. ¬øQu√© par√°metros cambiar√≠as o agregar√≠as?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\n\nls results/04.metabat/\nmetabat2 ‚Äì-help\n\n\n\n\n\n\n\nYa que corrimos Metabat2 vamos a ejecutar MaxBin2, pero primero necesitamos desactivar el ambiente:\n\n\n\n\n\n\nDesactiva el ambiente\n\n\n\nPara desactivar el ambiente debemos correr la siguiente linea:\nconda deactivate\n\n\n\n\nMaxBin2\nMaxBin2 agrupa los contigs de acuerdo a la informaci√≥n de cobertura, composici√≥n nucleot√≠dica y genes de marcadores de copia √∫nica.\nVamos a ejecutarlo, activemos el ambiente conda para maxbin.\n\n\n\n\n\n\nActivar ambiente para MaxBin2\n\n\n\n\nbetterlab\nconda activate metagenomics\n\n\n\n\n\n\nMaxBin2. Wu et al., 2014. https://doi.org/10.1186/2049-2618-2-26\n\n\nCrea el directorio para los resultados de MaxBin2\nmkdir -p results/05.maxbin\nAhora si, vamos a ejecutarlo.\n\nrun_MaxBin.pl -thread 4 -min_contig_length 1500 -contig results/02.ensambles/48hrs.fasta -out results/05.maxbin/48hrs_maxbin -abund results/03.profundidad/48hrs.mgh_depth.txt\n\n\n\n\n\n\nEjercicio:\n\n\n\n\n\n1. ¬øCu√°ntos bins se formaron?\n2. ¬øQu√© porcentaje de completitud y contaminaci√≥n tienen??\n\n\n\n\n\n\nSoluci√≥n\n\n\n\n\n\n\nls results/05.maxbin/*.fasta | wc -l\ncat results/05.maxbin/48hrs_maxbin.summary | column -t\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesactiva el ambiente\n\n\n\nconda deactivate\n\n\n\n\nVamb\nVAMB utiliza una combinaci√≥n de enfoques de aprendizaje profundo y t√©cnicas de agrupamiento bas√°ndose en sus patrones de composici√≥n de nucle√≥tidos y en la co-ocurrencia de sus frecuencias de cobertura.\n\n\n\n\n\n\nActiva el ambiente binning\n\n\n\n\nbetterlab\nconda activate binning\n\n\n\nVamos a correr vamb, pero primero crea el directorio de resultados\nmkdir -p results/06.vamb\nEjecutemos vamb:\nvamb --fasta results/02.ensambles/48hrs.fasta --jgi results/03.profundidad/48hrs.mgh_depth.txt --minfasta 500000 --outdir results/06.vamb/48hrs\n\n\n\n\n\n\nImportant\n\n\n\nSi quisieras recuperar los genomas de virus ¬øQu√© par√°metro cambiar√≠as?\n\n\n\n\n\n\n\n\nOtros programas para binning\n\n\n\nRecientemente se public√≥ COMEBin, que utiliza un enfoque distinto a lo que hemos usado en este tutorial. En el siguiente link encontrar√°s el manual y una explicaci√≥n general sobre su funcionamiento."
  },
  {
    "objectID": "Binning.html#refinamiento",
    "href": "Binning.html#refinamiento",
    "title": "Genomas a partir de metagenomas",
    "section": "Refinamiento",
    "text": "Refinamiento\nYa corrimos tres programas de binning, pero, recordemos que los agrupamientos pueden tener errores:\n\n\n\nContaminaci√≥n de bins\n\n\nPara disminuir la contaminaci√≥n e incrementar la completitud hay algunos programas que pueden ayudarnos. entre ellos est√°n Binning_refiner y DASTool.\n\nCheckM\nAntes de proceder al refinamiento es necesario tener claro c√≥mo se eval√∫a la completitud y contaminaci√≥n de los bins. Para esta evaluaci√≥n se usa CheckM que se ha convertido en una herramienta est√°ndar para la evaluaci√≥n de la calidad de genomas y MAGs, y es usada por la mayor√≠a de programas de refinamiento.\nPara hacer esta evaluaci√≥n, CheckM utiliza una serie de herramientas: tree organiza los genomas en un √°rbol de referencia. tree_qa eval√∫a la cantidad de genes marcadores filogen√©ticos y su ubicaci√≥n en el √°rbol. El comando lineage_set crea un archivo de marcadores espec√≠ficos de linaje, que se usa en el comando analyze para evaluar la integridad y contaminaci√≥n de los genomas. Finalmente, el comando qa genera tablas que resumen la calidad de los genomas.\n\n\n\nCheckM Workflow\n\n\nEn este taller no vamos a correr CheckM porque los programas de refinamiento que usaremos ya lo corren de forma interna, sin embargo, es √∫til correrlo para tener una idea clara sobre la calidad de los bins que obtengamos.\nTe dejamos la siguiente l√≠nea para que la uses en tus proyectos, adem√°s, en este repositorio encontrar√°s los scripts que pueden ser √∫tiles para extraer los bins que cumplan la calidad que buscas:\n#ejemplo de como correrlo con los bins de vamb\n#se debe activar el ambiente metagenomics\n#checkm lineage_wf results/06.vamb/48hrs/bins results/checkm/ -x fna -t 4  -f results/checkm/checkm_vamb_bins.txt\nY ahora si, a refinar los bins ‚Ä¶ ü•≥\n\n\nBinning_refiner\nBinning_refiner se enfoca en refinar y fusionar los bins para mejorar la integridad y reducir la contaminaci√≥n. Identifica bins que pueden representar el mismo genoma y los fusiona. Despu√©s elimina posibles contaminaciones, durante el proceso, Binning_refiner eval√∫a la calidad de los bins.\n\n\n\nBinning_refiner\n\n\nhttps://doi.org/10.1093/bioinformatics/btx086\nNecesitamos crear el directorio de resultados para binning_refiner y un directorio con los bins generados por cada programa\nmkdir -p results/07.binning_refiner/48hrsbins/{metabat,maxbin,vamb}\nAhora vamos a crear ligas simb√≥licas de los bins generados por cada herramienta.\n#metabat\ncd results/07.binning_refiner/48hrsbins/metabat/\n\nln -s ../../../04.metabat/*.fa .\n\n#maxbin\ncd ../maxbin/\nln -s ../../../05.maxbin/*.fasta .\n\n# vamb\ncd ../vamb/\nln -s ../../../06.vamb/48hrs/bins/*.fna .\n\n\n#regresar\ncd ../../\nAhora si, corramos Binning_refiner\nBinning_refiner -i 48hrsbins/ -p 48hrs\nY regresemos a nuestro directorio principal\ncd && cd taller_metagenomica_pozol/\nExploremos los resultados!\ncat results/07.binning_refiner/48hrs_Binning_refiner_outputs/48hrs_sources_and_length.txt\nRefined_bin     Size(Kbp)       Source\n48hrs_1 1535.49 48hrs_maxbin.004.fasta,metabat.5.fa,676.fna\n48hrs_2 1506.01 48hrs_maxbin.002.fasta,metabat.3.fa,6952.fna\n48hrs_3 1319.12 48hrs_maxbin.008.fasta,metabat.2.fa,28067.fna\n48hrs_4 1263.79 48hrs_maxbin.005.fasta,metabat.9.fa,3736.fna\n48hrs_5 1185.25 48hrs_maxbin.001.fasta,metabat.11.fa,15732.fna\n48hrs_6 1052.67 48hrs_maxbin.003.fasta,metabat.4.fa,15732.fna\n48hrs_7 557.49  48hrs_maxbin.006.fasta,metabat.1.fa,28990.fna\nAbre el archivo src/sankey.R o copia y pega este contenido en la consola de Rscript\n# Cargar las librerias\nlibrary(dplyr)\nlibrary(networkD3)\n\n# revisa tu ubicaci√≥n\ngetwd()\nsetwd(\"/home/alumno2/taller_metagenomica_pozol\")\n\n# Cargar los datos\nsankey_data &lt;- read.csv(\"results/07.binning_refiner/48hrs_Binning_refiner_outputs/48hrs_sankey.csv\")\n\n# Crear una lista de nodos √∫nicos\nnodes &lt;- data.frame(name = unique(c(sankey_data$C1, sankey_data$C2)))\n\n# Crear el dataframe de enlaces\nlinks &lt;- sankey_data %&gt;%\n  mutate(source = match(C1, nodes$name) - 1,\n         target = match(C2, nodes$name) - 1,\n         value = Length_Kbp) %&gt;%\n  select(source, target, value)\n\n# Crear el gr√°fico Sankey\nsankey_plot &lt;- sankeyNetwork(Links = links, Nodes = nodes,\n                             Source = \"source\", Target = \"target\",\n                             Value = \"value\", NodeID = \"name\",\n                             fontSize = 12, nodeWidth = 30)\n\n# Mostrar el gr√°fico\nsankey_plot\n\n# Guardar\nlibrary(htmlwidgets)\nsaveWidget(sankey_plot, file = \"48hrs_sankey_plot.html\")\n\n\n\nBinning_refiner sankey plot\n\n\n\n\nDASTool\nDASTool es una herramienta utilizada para mejorar la calidad de los bins. Eval√∫a la integridad, combina los resultados de diferentes bineadores y por consenso selecciona los mejores bins de cada herramienta. Una vez que DASTool ha seleccionado los mejores bins, realiza un proceso de refinamiento para optimizar los resultados.\n\n\n\nDASTool\n\n\nVamos a correr DASTool ‚Ä¶\nPrimero crea el directorio para los resultados\nmkdir -p results/08.dastool\nDASTool necesita como entrada un archivo tabular con informaci√≥n de los resultados de cada programa de binning.\nFasta_to_Contig2Bin.sh -i results/04.metabat/ -e fa &gt; results/08.dastool/48hrs_metabat.dastool.tsv\n\nFasta_to_Contig2Bin.sh -i results/05.maxbin/ -e fasta &gt; results/08.dastool/48hrs_maxbin.dastool.tsv\n\nFasta_to_Contig2Bin.sh -i results/06.vamb/48hrs/bins/ -e fna &gt; results/08.dastool/48hrs_vamb.dastool.tsv\n\nFasta_to_Contig2Bin.sh -i results/07.binning_refiner/48hrs_Binning_refiner_outputs/48hrs_refined_bins/ -e fasta &gt; results/08.dastool/48hrs_binningrefined.dastool.tsv\nYa que tenemos los archivos tsv podemos empezar con el refinamiento!! ü•≥\nDAS_Tool -i results/08.dastool/48hrs_metabat.dastool.tsv,results/08.dastool/48hrs_maxbin.dastool.tsv,results/08.dastool/48hrs_vamb.dastool.tsv,results/08.dastool/48hrs_binningrefined.dastool.tsv -l metabat,maxbin,vamb,binning_refined -c results/02.ensambles/48hrs.fasta -o results/08.dastool/48hrs -t 4 --write_bins"
  },
  {
    "objectID": "Binning.html#dereplicaci√≥n",
    "href": "Binning.html#dereplicaci√≥n",
    "title": "Genomas a partir de metagenomas",
    "section": "Dereplicaci√≥n",
    "text": "Dereplicaci√≥n\n\ndRep\nLa desreplicaci√≥n es el proceso de identificar conjuntos de genomas que son ‚Äúiguales‚Äù en una lista de genomas y eliminar todos los genomas excepto el ‚Äúmejor‚Äù de cada conjunto redundante. dRep es una herramienta √∫til para esto.\n\n\n\ndRep\n\n\nmkdir -p results/09.drep/bins\ncd results/09.drep/bins/\nfor i in $(ls ../../08.dastool/48hrs_DASTool_bins/*.fa) ; do name=$(basename $i .fa); cp $i $name\".fasta\" ; done\n\ncp ../../07.binning_refiner/48hrs_Binning_refiner_outputs/48hrs_refined_bins/*.fasta .\ncd && cd taller_metagenomica_pozol/\n\nexport PATH=/miniconda3/envs/metagenomics/bin:$PATH\ndRep dereplicate results/09.drep/ -d -comp 50 -con 10 --SkipSecondary -g results/09.drep/bins/*.fasta\n\n\n\ndRepWinningGenomes\n\n\nconda deactivate\n\n\n\n\n\n\nEjercicio 2\n\n\n\nAhora te toca a t√≠.\n\nRe√∫nanse en equipos y repliquen todo el flujo hasta este punto con la muestra que les toca.\nDiscutan cada resultado obtenido.\nEn la carpeta compartida de Drive busquen la presentaci√≥n para el Ejercicio 2, en la diapositiva correspondiente resuman sus resultados obtenidos para que los presenten.\n\nTiempo de actividad (1 hr)\nTiempo de presentaci√≥n de resultados (5 min por equipo)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "El pozol",
    "section": "",
    "text": "El pozol es un alimento √°cido, fermentado a partir de ma√≠z nixtamalizado. No se inocula y al final de su fermentaci√≥n tiene alta diversidad microbiana.\nüß¨üîäü¶† Imaginemos que se quiere impulsar la producci√≥n de esta bebida y para ello necesitan saber todo acerca de su naturaleza microbiana. Secuenciaron cuatro puntos de fermentaci√≥n de muestras que se obtuvieron en un mercado de Campeche. Las muestras se secuenciaron con Illumina NextSeq500 con lecturas pareadas de 75 pb.\nUna importante industria alimenticia los contacta como expertos en ecolog√≠a microbiana y les pide ayuda para descubrir los siguientes puntos:\n\n¬øQu√© actores microbianos est√°n presentes durante el proceso de fermentaci√≥n?\n¬øC√≥mo ocurre la bioconversi√≥n del ma√≠z durante la fermentaci√≥n, qui√©n participa y c√≥mo lo hace? ¬øQu√© funciones metab√≥licas est√°n ocurriendo?\n¬øHay potenciales pat√≥genos?\n¬øCambia la comunidad microbiana a lo largo del proceso?\n\n\n\n\n\n\n\nImportante\n\n\n\nComo las muestras contienen ma√≠z, es indispensable remover las lecturas que correspondan al genoma del ma√≠z, no hacerlo producir√° un ensamble muy fragmentado, mayoritariamente del ma√≠z y poco microbiano.\nEl autor del art√≠culo amablemente nos proporcion√≥ sus muestras libres del ma√≠z y el c√≥digo que us√≥ para ello est√° disponible en un repositorio p√∫blico de GitHub.\n\n\nEl art√≠culo: L√≥pez-S√°nchez et al., 2023. Analysing the dynamics of the bacterial community in pozol, a Mexican fermented corn dough. 10.1099/mic.0.001355"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Microbioma_del_pozol",
    "section": "",
    "text": "En este taller introductorio aprenderemos a reconstruir genomas a partir de metagenomas (MAGs), a clasificar los MAGs taxon√≥micamente y a predecir sus genes e inferir su metabolismo. Adem√°s, abordaremos un poco sobre el an√°lisis de amplicones del gene 16S ARNr."
  },
  {
    "objectID": "index.html#taller-introductorio-al-an√°lisis-de-metagenomas-centrado-en-genomas-y-an√°lisis-de-amplicones-del-gene-16s-arnr.",
    "href": "index.html#taller-introductorio-al-an√°lisis-de-metagenomas-centrado-en-genomas-y-an√°lisis-de-amplicones-del-gene-16s-arnr.",
    "title": "Microbioma_del_pozol",
    "section": "",
    "text": "En este taller introductorio aprenderemos a reconstruir genomas a partir de metagenomas (MAGs), a clasificar los MAGs taxon√≥micamente y a predecir sus genes e inferir su metabolismo. Adem√°s, abordaremos un poco sobre el an√°lisis de amplicones del gene 16S ARNr."
  },
  {
    "objectID": "index.html#temario",
    "href": "index.html#temario",
    "title": "Microbioma_del_pozol",
    "section": "Temario",
    "text": "Temario\nD√≠a 3\n\n\n\nHora\nTema\n\n\n\n\n9:00 - 9:30\nReconstrucci√≥n de genomas\n\n\n9:30 - 9:50\n¬øQu√© es el binning?\n\n\n9:50 - 10:00\nDescanso\n\n\n10:00 - 12:15\nReconstrucci√≥n de genomas\n\n\n12:15 - 12:30\nDescanso\n\n\n12:30 - 13:30\nEjercicio por equipo\n\n\n13:30 - 14:00\nDiscusi√≥n de resultados\n\n\n14:00 - 15:30\nComida\n\n\n15:30 - 16:00\nDereplicaci√≥n\n\n\n16:00 - 17:00\nClasificaci√≥n taxon√≥mica\n\n\n\nD√≠a 4\n\n\n\n\n\n\n\nHora\nTema\n\n\n\n\n9:00 - 10:00\nPredicci√≥n g√©nica\n\n\n10:00 - 10:30\nPl√°tica sobre el microbioma del chinicuil\n\n\n10:30 - 10:45\nDescanso\n\n\n10:45 - 12:00\nRbims\n\n\n12:00 - 12:15\nDescanso\n\n\n12:15 - 14:00\nOtras inferencias metab√≥licas y discusi√≥n de resultados\n\n\n14:00 - 15:30\nComida\n\n\n15:30 - 17:00\nVistazo a la reconstrucci√≥n viral y eucari√≥tica\n\n\n\nD√≠a 5\n\n\n\n\n\n\n\nHora\nTema\n\n\n\n\n9:00 - 10:00\nIntroducci√≥n al an√°lisis de amplicones\n\n\n10:00 - 10:30\nIntroducci√≥n a QIIME2\n\n\n10:30 - 10:45\nDescanso\n\n\n10:45 - 12:00\nImportaci√≥n de datos e inferencia de ASVs\n\n\n12:00 - 12:15\nDescanso\n\n\n12:15 - 14:00\nAsignaci√≥n taxon√≥mica, limpieza e inferencia filogen√©tica\n\n\n14:00 - 15:30\nComida\n\n\n15:30 - 17:00\nImportaci√≥n a R, gr√°ficos y diversidad"
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "Preparemos todo para el proyecto",
    "section": "",
    "text": "Reglas del juego\n\n\n\n\nEn este tutorial haremos el ejemplo corriendo la muestra de 48 hrs.\nSe formaran 6 equipos (2 de los tiempos 0, 9 y 24 hrs).\nLos equipos discutir√°n y presentar√°n sus resultados cuando se indique en el tutorial."
  },
  {
    "objectID": "Project.html#espacio-de-trabajo",
    "href": "Project.html#espacio-de-trabajo",
    "title": "Preparemos todo para el proyecto",
    "section": "Espacio de trabajo",
    "text": "Espacio de trabajo\n\nEntra a tu cuenta en el servidor y sit√∫ate en tu $HOME\nObten los datos y la estructura de tu directorio del proyecto corriendo lo siguiente:\n\ncd\ncp -r /home/alumno1/pozol_data/taller_metagenomica_pozol\n\nEntra al directorio del proyecto\n\ncd taller_metagenomica_pozol\n\n\n\n\n\n\nDirectorio de trabajo\n\n\n\nSi en alg√∫n momento te pierdes entre directorios, puedes regresar al espacio principal asi:\ncd && cd taller_metagenomica_pozol/\n\n\n\nLa presente pr√°ctica s√≥lo es una representaci√≥n del flujo de trabajo para el an√°lisis metagen√≥mico, sin embargo, no sustituye los manuales de cada programa y el flujo puede variar dependiendo del tipo de datos y pregunta de investigaci√≥n, de hecho para fines del taller, con frecuencia se utilizan las lineas de comando m√°s simples para eficientar tiempo y recursos, t√≥malo en cuenta.\n\nCada programa tiene una ayuda y un manual de usuario, es importante revisarlo y conocer cada par√°metro que se ejecute. En terminal se puede consultar el manual con el comando man y tambi√©n se puede consultar la ayuda con -h o --help, por ejemplo fastqc -h.\n\n\n\n\n\n\nImportant\n\n\n\nüß† Para tenerlo presente\nEn bioinform√°tica cualquier l√≠nea de comandos generar√° un resultado, de ah√≠ a que esos resultados sean correctos puede haber una gran diferencia.\nEn cada paso detente a revisar la informaci√≥n de cada programa, lee el manual, visita foros de ayuda y selecciona los argumentos que se ajusten a las necesidades de tus datos."
  },
  {
    "objectID": "metabolic.html",
    "href": "metabolic.html",
    "title": "Metabolismo",
    "section": "",
    "text": "Ahora que ya tenemos los bins refinados, queremos saber qu√© capacidades metab√≥licas poseen. Para ello es necesario predecir sus genes y asignarles funci√≥n.\n\nPROKKA\nProkka es una herramienta √∫til, usa diferentes programas para predecir genes, secuencias codificantes, tRNAs, rRNAs. Hace la traducci√≥n de CDS a amino√°cidos y asigna funciones usando varias bases de datos.\n\n\n\n\n\nPara correrlo vamos a activar el ambiente en el que se aloja.\n\n\n\n\n\n\nActiva el ambiente para PROKKA\n\n\n\nconda activate Prokka_Global\n\n\nTenemos el ambiente activo, ahora vamos a crear un directorio de resultados para prokka.\nmkdir -p results/11.prokka\nPara correrlo, podemos hacer un ciclo que nos permita anotar todos los bins.\nnohup\nfor FASTA in $(ls results/10.gtdbtk/bins/); do\n    LOCUSTAG=$(basename $FASTA .fasta)\n    prokka --locustag \"${LOCUSTAG}_Scaffold\" \\\n           --prefix $LOCUSTAG \\\n           --addgenes \\\n           --addmrna \\\n           --cpus 4 \\\n           --outdir \"results/11.prokka/$LOCUSTAG\" \\\n           \"results/10.gtdbtk/bins/$FASTA\"\ndone\n&gt; prokka.nohup &\n\n\n\n\n\n\nExplora\n\n\n\nMientras prokka se ejecuta en los bins que obtuviste, despliega la ayuda y discute:\n\n¬ø qu√© argumentos quitar√≠as o agregar√≠as?\nCu√°les te llamaron la atenci√≥n?\n\n\n\nDesactivemos el ambiente:\nconda deactivate\nAhora que tenemos las prote√≠nas predichas vamos a obtener m√°s anotaciones √∫tiles, usaremos kofam para esto.\n\n\nKofamScan\nKofamScan es una herramienta de anotaci√≥n, usa la base de datos KOfam de KEGG para obtener informaci√≥n sobre los genes que participan en diferentes rutas metab√≥licas.\nVamos a crear el directorio de resultados\nmkdir -p results/12.kofam\n\n\n\n\n\n\nEjemplo de como correr KOfamScan\n\n\n\nGHH\nKofamScan requiere mucho tiempo de ejecuci√≥n. Para efectos del taller nosotros ya lo corrimos y te proporcionaremos los resultados. Pero te dejamos el bloque de c√≥digo que usamos para este paso.\nfor FAA in $(ls results/11.prokka/*/*.faa); do\n    name=$(basename $FAA .faa)\n    exec_annotation $FAA \\\n        -o results/12.kofam/\"$name.txt\" \\\n        --report-unannotated \\\n        --cpu 4 \\\n        --tmp-dir results/12.kofam/\"tmp$name\" \\\n        -p /home/alumno1/kofam/db/profiles/ \\\n        -k /home/alumno1/kofam/db/ko_list\ndone\n# remover los directorios temporales\n#rm -r results/12.kofam/tmp*\n\n\nEstos resultados ya los tienes en el directorio results/12.kofam\nY ahora que ya tenemos los identificadores de KO para cada prote√≠na, vamos a filtrar y graficar el metabolismo de los bins.\n\n\nRbiMs\nRbiMs es un paquete de R muy √∫til para obtener la anotaci√≥n de cada KEGG ID y generar plots de esta informaci√≥n. Puede trabajar con anotaciones de KOFAM, Interpro o PFAM.\n\n\n\nRbiMs\n\n\nVamos al editor de Rstudio para correr RbiMs ‚ú®\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(rbims)\nlibrary(readxl)\n\nsetwd(\"/home/alumno1/taller_metagenomica_pozol/\")\n\n#A continuaci√≥n, leemos los resultados de KEGG y los mapeamos con la base de datos de KEGG:\n\npozol_table &lt;- read_ko(data_kofam = \"results/12.kofam/\") \n\npozol_mapp &lt;- mapping_ko(pozol_table)\n\n#Nos centraremos en las v√≠as metab√≥licas relacionadas con la obtenci√≥n de energ√≠a:\n\nOverview &lt;- c(\"Central Metabolism\", \"Carbon Fixation\", \"Nitrogen Metabolism\", \"Sulfur Metabolism\", \"Fermentation\", \"Methane Metabolism\")\nEnergy_metabolisms_pozol &lt;- pozol_mapp %&gt;%\n  drop_na(Cycle) %&gt;%\n  get_subset_pathway(rbims_pathway, Overview) \n\n#Visualizamos los datos con un gr√°fico de burbujas:\n\nplot_bubble(tibble_ko = Energy_metabolisms_pozol,\n            x_axis = Bin_name, \n            y_axis = Pathway_cycle,\n            analysis = \"KEGG\",\n            calc = \"Percentage\",\n            range_size = c(1, 10),\n            y_labs = FALSE,\n            x_labs = FALSE)  \n\n#A√±adiremos metadatos, como la taxonom√≠a:\n\nMetadatos &lt;- read_delim(\"10.gtdbtk/Metadatos.txt\", delim = \"\\t\")\n\n#Y generaremos un gr√°fico de burbujas con metadatos:\n\nplot_bubble(tibble_ko = Energy_metabolisms_pozol,\n            x_axis = Bin_name, \n            y_axis = Pathway_cycle,\n            analysis = \"KEGG\",\n            data_experiment = Metadatos,\n            calc = \"Percentage\",\n            color_character = Class,\n            range_size = c(1, 10),\n            y_labs = FALSE,\n            x_labs = FALSE) \n\n# Exploraci√≥n de una V√≠a Espec√≠fica\n# podemos explorar una sola v√≠a, como el ‚ÄúSecretion system,‚Äù y crear un mapa de calor para visualizar los genes relacionados con esta v√≠a:\n\nSecretion_system_pozol &lt;- pozol_mapp %&gt;%\n  drop_na(Cycle) %&gt;%\n  get_subset_pathway(Cycle, \"Secretion system\")\n\n#Y, finalmente, generamos un mapa de calor:\n\nplot_heatmap(tibble_ko = Secretion_system_pozol, \n             y_axis = Genes,\n             analysis = \"KEGG\",\n             calc = \"Binary\")\n\n#Tambi√©n podemos agregar metadatos para obtener una visi√≥n m√°s completa:\n\nplot_heatmap(tibble_ko = Secretion_system_pozol, \n             y_axis = Genes,\n             data_experiment = Metadatos,\n             order_x = Phylum,\n             analysis = \"KEGG\",\n             calc = \"Binary\")\nplot_heatmap(tibble_ko = Secretion_system_pozol, \n             y_axis = Genes,\n             data_experiment = Metadatos,\n             order_y = Pathway_cycle,\n             order_x = Phylum,\n             analysis = \"KEGG\",\n             calc = \"Binary\")\nExtras???\n\n\nAntismash\nhttps://carpentries-incubator.github.io/genome-mining/02-antismash/index.html\nhttps://carpentries-incubator.github.io/genome-mining/03-antiSMASHdb-MiBiG/index.html"
  },
  {
    "objectID": "gtdbtk.html",
    "href": "gtdbtk.html",
    "title": "Asignaci√≥n taxon√≥mica",
    "section": "",
    "text": "GTDB-tk\nGTDB-Tk es una herramienta que asigna taxonom√≠a a genomas utilizando la base de datos GTDB (Genome Taxonomy Database). Basado en √°rboles filogen√©ticos y medidas de ANI (Average Nucleotide Identity), GTDB-Tk clasifica genomas bacterianos y arqueanos, proporciona una taxonom√≠a coherente y actualizada. Se utiliza mucho en el an√°lisis de genomas y metagenomas.\n\n\n\nGTDB-tk Workflow\n\n\nRecordemos que ya tenemos un set de bins refinados y desreplicados. Ahora vamos a asignarles identidad taxon√≥mica, para ello vamos a correr GTDB-tk\n\n\n\n\n\n\nActiva el ambiente de gtdbtk\n\n\n\nconda activate gtdbtk-2.1.1\n\n\nCrea el directorio de resultados para gtdbtk, copia y renombra los bins:\nbash src/copiar_renombrar_bins_para_gtdb.sh\nAhora si, vamos a correrlo\npip install numpy==1.19.5\n\nnohup gtdbtk classify_wf --genome_dir results/10.gtdbtk/bins/ --out_dir results/10.gtdbtk/ --cpus 4 -x fasta &gt; gtdbtk.nohup &\nNo olvides desactivar el ambiente\nconda deactivate\nDespu√©s de ejecutar GTDB-tk, continuaremos en R para visualizar los datos.\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nGTDBK &lt;- read.table(\"results/10.gtdbtk/gtdbtk.bac120.summary.tsv\", \n  sep = \"\\t\", header = TRUE, na.strings = \"\", stringsAsFactors = FALSE) %&gt;%\n  as_tibble()\n\n#El archivo contiene informaci√≥n sobre la clasificaci√≥n taxon√≥mica de los bins.\n#Continuamos limpiando y transformando los datos:\n\npozol_gtdbtk &lt;- GTDBK %&gt;%\n  select(user_genome, classification) %&gt;%\n  separate(classification, c(\"Domain\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\"), sep = \";\") %&gt;%\n  rename(Bin_name = user_genome) %&gt;%\n  unite(Bin_name_2, c(\"Bin_name\", \"Phylum\"), remove = FALSE) %&gt;%\n  select(Bin_name, Domain, Phylum, Class, Order, Family, Genus, Species)\n\n#Guardamos los datos en un archivo de metadatos:\nwrite.table(pozol_gtdbtk, file = \"results/10.gtdbtk/Metadatos.txt\", sep = \"\\t\", quote = FALSE, row.names = FALSE, col.names = TRUE)\n\n#Visualizaci√≥n de Datos Creamos un gr√°fico de barras  que muestra la distribuci√≥n taxon√≥mica de los bins:\n\nGTDBtk_barplot &lt;- pozol_gtdbtk %&gt;%\n  count(Phylum, Genus) %&gt;%\n  rename(Number_of_MAGs = n) %&gt;%\n  ggplot(aes(x = Phylum, y = Number_of_MAGs, fill = Genus)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  theme_minimal()\n\nGTDBtk_barplot\n\n\n\n\n\n\nDiscusi√≥n\n\n\n\nEn equipos revisen los resultados generados por GTDB-tk y propongan un plan para mejorar la identificaci√≥n taxon√≥mica, qu√© har√≠an para darle m√°s soporte a estos resultados?"
  },
  {
    "objectID": "Amplicones16S.html",
    "href": "Amplicones16S.html",
    "title": "Amplicones 16S",
    "section": "",
    "text": "An√°lisis de amplicones\nEl an√°lisis de amplicones, tambi√©n llamado meta-taxonom√≠a y mal llamado metagen√≥mica. Se usa ampliamente para conocer la diversidad taxon√≥mica de un ambiente. Se amplifica y secuenc√≠a una regi√≥n de cada organismo que se encuentra en la comunidad. Estos marcadores pueden ser los genes ribosomales 16S, 18S, ITS o el COI, etc.\n\n\n\n\n\nExisten diversas herramientas para analizar este tipo de datos como Mothur, USEARCH, VSEARCH, Deblur, AMPtk, DADA2 en R, QIIME2, etc.\nEn este taller vamos a usar Qiime2\n\n\nQIIME2\nQiime2 (Quantitative Insights Into Microbial Ecology) es un pipeline desarrollado para el an√°lisis de metataxonom√≠a (Bolyen et al., 2019). Contiene herramientas para limpiar secuencias, agrupar, asignar taxonom√≠a, reconstruir filogenias, inferir m√©tricas de diversidad, abundancia diferencial, etc. Es de c√≥digo abierto, posee una interfaz gr√°fica amigable, mucha documentaci√≥n, tutoriales y foros de ayuda.\n\n\n\n\n\n\n\nLimpieza\nAunque dentro de qiime se puede hacer la limpieza de los adaptadores, tambi√©n podemos hacerlo por fuera con cutadapt.\nVamos a crear el directorio de resultados de cutadapt\nmkdir -p results/01.cutadapt\n\nout=\"results/01.cutadapt\"\n\nfor FILE in $(ls data/*.gz | sed 's/_.*//' | sed 's/data\\///' | sort -u); do\n  cutadapt -m 200 --pair-filter any --no-indels \\\n    -g CCTACGGGNGGCWGCAG -G GACTACHVGGGTATCTAATCC -Z -j 4 \\\n    -o $out/\"${FILE}_1.fastq.gz\" -p $out/\"${FILE}_2.fastq.gz\" data/\"${FILE}_R1.fastq.gz\" data/\"${FILE}_R2.fastq.gz\"\ndone\n\n\nCrear Manifest\nAhora que tenemos los fastq sin adaptadores vamos a importarlos a QIIME2, para esto necesitamos crear un archivo manifest con la informaci√≥n de la ubicaci√≥n de los datos a analizar, para ello ejecuta el script `src/create_manifest.sh`\nbash src/create_manifest.sh\nEl contenido es el siguiente:\n#!/usr/bin/bash\n#Create manifest file\n\ncd results/01.cutadapt\n\nid=$(ls *.gz |  sed 's/_.*//g' | sort -u)\n\necho -e \"sample-id\"\",\"\"absolute-filepath\"\",\"\"direction\" &gt; ../../data/manifest.csv\n\nfor sample in $id; do\n        source=$(awk -F\"\\t\" -v sample=\"$sample\" '$1 == sample {print $1}' ../../data/metadata.tsv)\n        r1=$(echo -e $sample\"_1.fastq.gz\"\",\"\"forward\")\n        r2=$(echo -e $sample\"_2.fastq.gz\"\",\"\"reverse\")\n        path=$(pwd)\n        echo -e $source\",\"$path\"/\"$r1 &gt;&gt; ../../data/manifest.csv\n        echo -e $source\",\"$path\"/\"$r2 &gt;&gt; ../../data/manifest.csv\ndone\n\n\nImportar los datos\nBien, ahora ya tenemos el archivo manifest que es necesario para importar a QIIME2. Vamos a hacerlo ‚Ä¶ pero primero a crear el directorio de resultados de qiime.\nmkdir -p results/02.qiime\n#importar data\nqiime tools import --type 'SampleData[PairedEndSequencesWithQuality]'\\\n --input-path data/manifest.csv \\\n --output-path results/02.qiime/01.demux.qza\\\n --input-format 'PairedEndFastqManifestPhred33'\n#convertir qza a qzv\nqiime demux summarize --i-data results/02.qiime/01.demux.qza --o-visualization results/02.qiime/01.demux.qzv\nDescarga el archivo qzv en tu computadora y vizualizalo en la p√°gina Qiime2view. Veras algo como esto:\n\nYa que visualizamos la calidad y el n√∫mero de lecturas que tienen las muestras, procederemos a eliminar el ruido y hacer el agrupamiento con DADA2.\n\n\nEliminaci√≥n de ruido y agrupamiento con Dada2\n\n\n\n\n\n\nDiscutamos\n\n\n\nAntes de comenzar el denoising , veamos la ayuda.\n\n\nqiime dada2 denoise-paired --i-demultiplexed-seqs --help\nComo habr√°s notado, es necesario tomar decisiones basadas en la calidad de nuestras lecturas para definir los valores de truncado. Es muy importante dener en cuenta la longitud de las lecturas y del amplic√≥n deseado para tener una idea de la longitud del sobrelape que se obtendr√≠a.\nToma en cuenta lo siguiente:\n(longitud lectura Fordware) + (longitud lectura Reverse) ‚àí (longitud del amplicon) ‚àí \n(longitud lectura Fordware ‚àí --p-trunc-len-f value) ‚àí (longitud lectura Reverse ‚àí --p-trunc-len-r value) \n= sobrelape\nPara hacer el denoising y agrupamiento con DADA2 ejecutemos lo siguiente:\nqiime dada2 denoise-paired --i-demultiplexed-seqs results/02.qiime/01.demux.qza  --p-trunc-len-f 280 --p-trunc-len-r 250  --o-representative-sequences results/02.qiime/03.rep-seqs_v1.qza --o-table results/02.qiime/03.feature-table_v1.qza --o-denoising-stats results/03.denoising-stats_v1.qza --p-n-threads 10\nVeamos los estad√≠sticos de lo que se pudo agrupar.\nqiime tools export --input-path results/03.denoising-stats_v1.qza --output-path results/03.denoising-stats_v1\n\n\n\n\n\n\nEjercicio 1\n\n\n\nRe√∫nanse en equipos, revisen la ayuda, discutan y generen una versi√≥n 2 de denoising. Obtengan sus estad√≠sticas, comparen los resultados y expongan sus conclusiones en la presentaci√≥n que corresponde.\nhttps://drive.google.com/drive/folders/1iKfhMz_JdfImmsCmkPg10r-NC-nrzhQ4?usp=sharing\n\n\n\n\n\n\nTip\n\n\n\nPara comparar puedes correr la siguiente l√≠nea\nhead results/02.qiime/03.denoising-stats_v*/stats.tsv\n\n\n\n\n\n\nAsignaci√≥n taxon√≥mica\nEn qiime podemos hacer la anotaci√≥n taxon√≥mica usando diferentes aproximaciones. Nosotros usaremos sklearn para esto, en el taller usaremos una base datos ya entrenada para las regiones V3-V4. El c√≥digo para que t√∫ entrenes la base de datos de acuerdo a la regi√≥n que te interesa lo puedes encontrar en esta p√°gina. O en la p√°gina de QIIME suele estar una base ya entrenada para la regi√≥n V4 que puedes descargar aqu√≠.\nVamos a hacer la asignaci√≥n taxon√≥mica:\nqiime feature-classifier classify-sklearn \\\n  --i-classifier data/dbs/classifier_silva_138_trained.qza \\\n  --i-reads results/02.qiime/03.rep-seqs_v1.qza \\\n  --o-classification results/02.qiime/taxonomy.qza --p-n-jobs 10\nY podemos generar archivos para visualizar los resultados\n# Visualizar la taxonom√≠a\nqiime metadata tabulate \\\n  --m-input-file results/02.qiime/taxonomy.qza \\\n  --o-visualization results/02.qiime/taxonomy.qzv\n# Visualizar las secuencias representativas\nqiime feature-table tabulate-seqs \\\n--i-data results/02.qiime/03.rep-seqs_v1.qza \\\n--o-visualization results/02.qiime/03.rep-seqs_v1.qza.qzv\n# visualizar la tabla de conteos\nqiime feature-table summarize \\\n--i-table results/02.qiime/03.feature-table_v1.qza \\\n--o-visualization results/02.qiime/03.feature-table_v1.qzv\nDescarga los archivos qzv en tu computadora y visual√≠zalos en la p√°gina Qiime2view\nOpcionalmente te dejamos un link con scripts para hacer filtros subsecuentes, como eliminar secuencias que son potenciales artefactos, remover cloroplastos o mitocondria.\n\n\nVisualicemos en R\nY ahora si viene lo divertido, vamos a visaulizar nuestros resultados ‚Ä¶\nAbre tu editor en Rstudio\nImportemos los datos de qiime a R como un objeto Phyloseq\nlibrary(phyloseq)\nlibrary(qiime2R)\n\n#create phyloseq object\n\nps &lt;- qza_to_phyloseq(\n  features = \"../results/04.qiime/ASV_table_filter_freq218_emc.qza\",\n  #tree = \"../results/04.qiime/rooted-tree-iqtree.qza\",\n  taxonomy = \"../results/04.qiime/taxonomy.qza\",\n  metadata = \"../data/metadata.tsv\")\nObtengamos informaci√≥n del objeto phyloseq\nlibrary(devtools)\nlibrary(microbiome)\n\n#check data\nmicrobiome::summarize_phyloseq(ps)\nHagamos un chequeo r√°pido del esfuerzo de muestreo\nlibrary(vegan)\nmat &lt;- as(t(otu_table(ps)), \"matrix\")\nraremax &lt;- min(rowSums(mat))\n\nsystem.time(rarecurve(mat, step = 1000, sample = raremax, \n                                                  col = \"purple4\", label = TRUE))\nVeamos el objeto phyloseq en un objeto Ampvis\nlibrary(ampvis2)\nlibrary(dplyr)\n\notu_table_ampvis &lt;- data.frame(OTU = rownames(phyloseq::otu_table(ps3)@.Data),\n                               phyloseq::otu_table(ps3)@.Data,\n                               phyloseq::tax_table(ps3)@.Data,\n                               check.names = FALSE)\nmeta_data_ampvis &lt;- data.frame(phyloseq::sample_data(ps3),\n                               check.names = FALSE\n)\n# change index to SampleID\nmeta_data_ampvis &lt;- meta_data_ampvis %&gt;% rownames_to_column(var = \"SampleID\")\n# finalmente generamos el objeto ampvis\nav2 &lt;- amp_load(otu_table_ampvis, meta_data_ampvis)\nVisualicemos las abundancias en un heatmap\n#heatmap\nGenus_av2_abundance_plot &lt;- amp_heatmap(av2,\n            facet_by = \"sampleID\",\n            plot_values = TRUE,\n            plot_values_size = 4,\n            tax_show = 45,\n            tax_aggregate = \"Genus\",\n            tax_add = c(\"Kingdom\", \"Phylum\"),\n            plot_legendbreaks = c(1, 5, 5))\nGenus_av2_abundance_plot\nDiversidad alfa\nDiversidad beta\n\n\nRecursos externos\nSi quieres profundizar en el an√°lisis de amplicones te recomendamos visitar el canal de youtube y/o los cursos y asesorias de microbioma-lab."
  }
]